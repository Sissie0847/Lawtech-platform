"""
ç¤¾ç¾¤è¿è¥æ–‡æ¡ˆç”Ÿæˆæ¨¡å—
æ ¹æ®å…¥åº“æ–°é—»ç”Ÿæˆ"è°ˆèµ„å‹"æ—©æŠ¥å¯¼è¯»æ–‡æ¡ˆ
"""

import requests
import json
from config import BAILIAN_API_KEY, BAILIAN_MODEL


def generate_community_copy(top_articles: list, other_titles: list = None) -> dict:
    """
    ç”Ÿæˆç¤¾ç¾¤è¿è¥æ–‡æ¡ˆ
    
    top_articles: å‰5æ¡æ ¸å¿ƒæ–‡ç« ï¼ŒåŒ…å«æ ‡é¢˜å’Œå†…å®¹
    other_titles: å…¶ä½™æ–‡ç« çš„æ ‡é¢˜åˆ—è¡¨
    
    è¿”å›: {"copy": "æ–‡æ¡ˆå†…å®¹", "analysis": "å…³è”åˆ†æ"}
    """
    
    # æ„å»ºå®Œæ•´æ–‡ç« å†…å®¹ï¼ˆä¸æˆªæ–­ï¼Œä¿ç•™å…¨éƒ¨ä¿¡æ¯ï¼‰
    articles_text = ""
    for i, article in enumerate(top_articles[:5], 1):
        title = article.get("æ ‡é¢˜", "")
        content = article.get("åŸæ–‡å†…å®¹", "") or article.get("AIæ€»ç»“", "")
        source = article.get("æ¥æºåç§°", "")
        ai_reason = article.get("AIç†ç”±", "")
        
        articles_text += f"\nâ”â”â” ç¬¬{i}æ¡ â”â”â”\n"
        articles_text += f"ğŸ“° æ ‡é¢˜ï¼š{title}\n"
        if source:
            articles_text += f"ğŸ“Œ æ¥æºï¼š{source}\n"
        if ai_reason:
            articles_text += f"ğŸ’¡ AIæ¨èç†ç”±ï¼š{ai_reason}\n"
        articles_text += f"ğŸ“„ å…¨æ–‡ï¼š\n{content}\n"
    
    # æ„å»ºå…¶ä»–æ ‡é¢˜
    other_text = ""
    if other_titles:
        other_text = "\n".join([f"- {t}" for t in other_titles[:10]])
    
    prompt = f"""ä½ æ˜¯ä¸€ä½æ·±è€•"æ³•å¾‹ç§‘æŠ€"é¢†åŸŸçš„èµ„æ·±å†…å®¹è¿è¥ä¸“å®¶ï¼Œæ­£åœ¨ä¸ºç¤¾ç¾¤æ’°å†™ã€æ¯æ—¥ç²¾é€‰å¯¼è¯»ã€‘ã€‚

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“¥ ä»Šæ—¥å…¥åº“æ–‡ç« ï¼ˆå®Œæ•´å†…å®¹ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{articles_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ å…¶ä»–ç›¸å…³èµ„è®¯æ ‡é¢˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{other_text if other_text else "æ— "}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ ä½ çš„ä»»åŠ¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Role
ä½ æ˜¯ä¸€åã€æ³•å¾‹ç§‘æŠ€ç¤¾ç¾¤ã€‘çš„èµ„æ·±æƒ…æŠ¥å®˜ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å†™ä¸€ä»½ã€ç¤¾ç¾¤æ—©æŠ¥å¯¼è¯»ã€‘ã€‚

# Rules (æ ¸å¿ƒåŸåˆ™)
1.  **æç®€ä¸»ä¹‰**ï¼šä¸è¦å¯’æš„ï¼Œç›´æ¥è¿›å…¥ä¸»é¢˜ã€‚
2.  **æ‹’ç»AIå‘³**ï¼šä¸¥ç¦ä½¿ç”¨"ç»¼ä¸Šæ‰€è¿°"ã€"è®©æˆ‘ä»¬æ‹­ç›®ä»¥å¾…"ã€"é‡ç£…å‘å¸ƒ"ã€‚
3.  **ç²¾å‡†ç»“æ„**ï¼šä¸¥æ ¼éµå®ˆä¸‹æ–¹çš„ä¸‰æ®µå¼ç»“æ„ã€‚

# Writing Logic (å†™ä½œé€»è¾‘)

## ç¬¬ä¸€æ®µï¼šå…¨æ™¯æ‰«æ (The Scope)
* **æ ¼å¼è¦æ±‚**ï¼šä»¥ `é€Ÿé€’ | ` å¼€å¤´ã€‚
* **å†…å®¹**ï¼šé˜…è¯»æ‰€æœ‰èµ„è®¯ï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬ä»Šå¤©çš„èµ„è®¯æ¶æ„ã€‚
* **å¥å¼æ¨¡æ¿**ï¼š`é€Ÿé€’ | ä»Šå¤©çš„åŠ¨æ€æ¶µç›–äº†ä»[é¢†åŸŸA]çš„[å…·ä½“äº‹ä»¶]åˆ°[é¢†åŸŸB]çš„[å…·ä½“äº‹ä»¶]ï¼Œ[4ä¸ªå­—çš„æ€»ç»“è¯]ã€‚`

## ç¬¬äºŒæ®µï¼šçƒ­è®®èšç„¦ (The Trend)
* **åˆ†æé€»è¾‘**ï¼šè¯·è§‚å¯Ÿæ‰€æœ‰èµ„è®¯ä¸­ï¼Œ**å“ªä¸ªæ–¹å‘/è¯é¢˜å‡ºç°çš„æ–°é—»æ•°é‡æœ€å¤šï¼Ÿ**
* **å†…å®¹**ï¼šæŒ‡å‡ºä»Šå¤©æœ€é›†ä¸­çš„é£å‘æ˜¯ä»€ä¹ˆï¼Œå¹¶åˆ†æä¸ºä»€ä¹ˆå¤§å®¶éƒ½åœ¨æè¿™ä¸ªã€‚
* **æ ¼å¼è¦æ±‚**ï¼š`ä¸è¿‡ä»Šå¤©æœ€é›†ä¸­çš„æ–¹å‘è¿˜æ˜¯` + `**[å…±åŒè¯é¢˜]**` + `ï¼Œ` + `[æ•°é‡]æ¡èµ„è®¯éƒ½æ¶‰åŠåˆ°äº†æ­¤è¯é¢˜` + ` â€”â€” ` + `**ä½ çš„æ·±åº¦åˆ†æï¼ˆåŠ ç²—ï¼‰**` + `ã€‚`

## ç¬¬ä¸‰æ®µï¼šä»Šæ—¥åˆ’é‡ç‚¹ (The Highlights)
* **é€‰ææ ‡å‡†**ï¼šæŒ‘é€‰ 3 æ¡æœ€å…·å®åŠ¡ä»·å€¼æˆ–å•†ä¸šäº®ç‚¹çš„æ–°é—»ã€‚
* **æ ¼å¼æ ‡é¢˜**ï¼š`ä»Šæ—¥åˆ’é‡ç‚¹ï¼š`
* **åˆ—è¡¨æ ¼å¼**ï¼š`[æ ‡é¢˜]` + `ï¼š` + `[äººè¯ç‚¹è¯„]`
* **ç‚¹è¯„è¦æ±‚**ï¼šä¸è¦å¤è¿°æ–°é—»ï¼è¦å†™å‡ºä½ çš„**æƒ…ç»ªã€æ€åº¦æˆ–åˆ¤æ–­**ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
    "copy": "æŒ‰ä¸Šè¿°ä¸‰æ®µå¼ç»“æ„æ’°å†™çš„å®Œæ•´å¯¼è¯»ï¼ˆ100-150å­—ï¼‰",
    "guide": "å¼•å¯¼è¯­ï¼ˆå¦‚ï¼šå®Œæ•´å†…å®¹è§ä»Šæ—¥æ¨é€ğŸ‘‡ï¼‰",
    "highlights": ["äº®ç‚¹1", "äº®ç‚¹2", "äº®ç‚¹3"],
    "analysis": "ä½ è§‚å¯Ÿåˆ°çš„è¡Œä¸šè¶‹åŠ¿æˆ–å…³è”ï¼ˆä¾›å‚è€ƒï¼‰"
}}

æ³¨æ„ï¼šç›´æ¥è¾“å‡ºæ ¸å¿ƒä¿¡æ¯ï¼Œä¸è¦ç”¨"ä»Šå¤©æˆ‘ä»¬æ¥çœ‹çœ‹"è¿™ç±»ç©ºæ´å¼€åœºã€‚"""

    headers = {
        "Authorization": f"Bearer {BAILIAN_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": BAILIAN_MODEL,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 500,
        "response_format": {"type": "json_object"}
    }
    
    try:
        response = requests.post(
            "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        
        data = response.json()
        
        if "choices" in data and len(data["choices"]) > 0:
            result_str = data["choices"][0]["message"]["content"]
            try:
                result = json.loads(result_str)
                copy_text = result.get("copy", "")
                guide_text = result.get("guide", "å®Œæ•´å†…å®¹è§ä»Šæ—¥æ¨é€ğŸ‘‡")
                highlights = result.get("highlights", [])
                
                # æ‹¼æ¥å®Œæ•´æ–‡æ¡ˆï¼šæ­£æ–‡ + æ¢è¡Œ + å¼•å¯¼è¯­
                full_copy = f"{copy_text}\n\n{guide_text}"
                
                return {
                    "copy": full_copy,
                    "copy_only": copy_text,
                    "guide": guide_text,
                    "highlights": highlights,
                    "analysis": result.get("analysis", ""),
                    "success": True
                }
            except json.JSONDecodeError:
                return {
                    "copy": result_str,
                    "copy_only": result_str,
                    "guide": "",
                    "highlights": [],
                    "analysis": "",
                    "success": True
                }
        
        return {"copy": "", "analysis": "", "success": False, "error": "AI è¿”å›ä¸ºç©º"}
        
    except requests.exceptions.Timeout:
        return {"copy": "", "analysis": "", "success": False, "error": "è¯·æ±‚è¶…æ—¶"}
    except requests.exceptions.RequestException as e:
        return {"copy": "", "analysis": "", "success": False, "error": f"ç½‘ç»œé”™è¯¯: {e}"}
    except Exception as e:
        return {"copy": "", "analysis": "", "success": False, "error": f"ç”Ÿæˆå¤±è´¥: {e}"}


def test_generate():
    """æµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    test_articles = [
        {
            "æ ‡é¢˜": "LexArbitra æ¨å‡ºå…è´¹æ¡ˆä¾‹æ³•æœç´¢å¼•æ“", 
            "åŸæ–‡å†…å®¹": "LexArbitra å®£å¸ƒæ¨å‡ºå…¨æ–°çš„å…è´¹æ¡ˆä¾‹æ³•æœç´¢å¼•æ“ã€‚è¯¥å·¥å…·å·²è¦†ç›–ç¾å›½è”é‚¦åŠ 8 ä¸ªå·çš„æ¡ˆä¾‹æ•°æ®ï¼Œå¹¶æ‰¿è¯ºæ°¸ä¹…å…è´¹ä½¿ç”¨ã€‚åˆ›å§‹äººè¡¨ç¤ºï¼Œè¿™æ˜¯ä¸ºäº†æ‰“ç ´æ³•å¾‹ä¿¡æ¯çš„ä»˜è´¹å£å’ï¼Œè®©æ›´å¤šäººèƒ½å¤Ÿå¹³ç­‰è·å–æ³•å¾‹çŸ¥è¯†ã€‚",
            "æ¥æºåç§°": "Legal IT Insider",
            "AIç†ç”±": "æä¾›å…è´¹æ³•å¾‹æœç´¢å·¥å…·ï¼Œå…·æœ‰æ™®æƒ ä»·å€¼"
        },
        {
            "æ ‡é¢˜": "æ³•å¾‹å·¥ç¨‹å¸ˆï¼šåœ¨æŠ€æœ¯ä¸æ³•å¾‹çš„äº¤æ±‡å¤„æ¶æ¡¥", 
            "åŸæ–‡å†…å®¹": "Jeannique Swiegers æ˜¯æ–°ä¸€ä»£æ³•å¾‹å·¥ç¨‹å¸ˆçš„ä»£è¡¨ï¼Œå¥¹å°†æ³•å¾‹è¯­è¨€è½¬åŒ–ä¸ºæ™ºèƒ½ç³»ç»Ÿã€‚å¥¹è®¤ä¸ºæ³•å¾‹ AI çš„æœªæ¥åœ¨äºåä½œè€Œéæ›¿ä»£ï¼Œç³»ç»Ÿå¤„ç†å¸¸è§„å·¥ä½œï¼Œäººç±»ä¸“æ³¨åˆ¤æ–­ã€‚'æ³•å¾‹çš„ä¸‹ä¸€ä¸ªé˜¶æ®µå°†ç”±äººã€æŠ€æœ¯å’Œæ•°æ®ä¹‹é—´çš„åä½œå®šä¹‰ã€‚'",
            "æ¥æºåç§°": "Legal IT Insider",
            "AIç†ç”±": "æ·±åº¦æ¢è®¨æ³•å¾‹å·¥ç¨‹å¸ˆè§’è‰²ä¸äººæœºåä½œ"
        },
        {
            "æ ‡é¢˜": "Word æ’ä»¶å®ç° AI æ¡æ¬¾é€è¯ä¿®è®¢", 
            "åŸæ–‡å†…å®¹": "å¼€å‘è€… yuch85 æ„å»ºäº†ä¸€æ¬¾ Word æ’ä»¶ï¼Œèƒ½å°† LLM ç”Ÿæˆçš„æ¡æ¬¾ä¿®æ”¹è½¬æ¢ä¸ºè¿½è¸ªä¿®è®¢æ¨¡å¼ã€‚diff å¼•æ“å¯ç”Ÿæˆé€è¯ä¿®æ”¹è€Œéæ•´æ®µæ›¿æ¢ã€‚å¼€å‘è€…è®¡åˆ’ä»¥ AGPL åè®®å¼€æºæ ¸å¿ƒå¼•æ“ã€‚",
            "æ¥æºåç§°": "Reddit æ³•å¾‹ç§‘æŠ€ç¤¾åŒº",
            "AIç†ç”±": "å±•ç°æ³•å¾‹ç§‘æŠ€å¼€æºåˆ›æ–°å®è·µ"
        },
    ]
    
    other_titles = [
        "å¾‹å¸ˆå…¼å¼€å‘è€…å¯»æ±‚æ³•å¾‹ç§‘æŠ€å•†ä¸šæœºä¼š",
        "éŸ©å›½æ•°æ®åˆè§„å®åŠ¡ä¸­çš„ä¸ƒä¸ªå…³é”®ç»†èŠ‚",
        "åˆ‘äº‹è¾©æŠ¤å¾‹æ‰€å¯»æ±‚è‡ªåŠ¨åŒ–å¤–å‘¼æœåŠ¡"
    ]
    
    result = generate_community_copy(test_articles, other_titles)
    
    print("=" * 60)
    print("ğŸ“ ç”Ÿæˆçš„ç¤¾ç¾¤æ–‡æ¡ˆï¼š")
    print("-" * 60)
    print(result.get("copy", ""))
    print("-" * 60)
    
    highlights = result.get("highlights", [])
    if highlights:
        print("\nâœ¨ äº®ç‚¹æç‚¼ï¼š")
        for h in highlights:
            print(f"  â€¢ {h}")
    
    print("\nğŸ” è¶‹åŠ¿åˆ†æï¼š")
    print(result.get("analysis", ""))
    print("=" * 60)


if __name__ == "__main__":
    test_generate()

